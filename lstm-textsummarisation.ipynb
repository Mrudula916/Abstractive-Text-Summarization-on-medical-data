{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-04T21:02:21.323802Z","iopub.execute_input":"2022-05-04T21:02:21.324094Z","iopub.status.idle":"2022-05-04T21:02:21.331177Z","shell.execute_reply.started":"2022-05-04T21:02:21.324064Z","shell.execute_reply":"2022-05-04T21:02:21.330180Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Converting jasonl files to csv\nimport csv\nimport json\nimport time\n\ntesting_csv = \"testing_text.csv\"\ntraining_csv = \"training_text.csv\"\nvalidating_csv = \"validating_text.csv\"\n\ntesting = \"testing_summary.jsonl\"\ntraining = \"training_summary.jsonl\"\nvalidating = \"validation_summary.jsonl\"\n\ndef writeTocsv(data):\n    with open(validating_csv, 'a+', encoding='UTF8', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(data)\n\nwith open(validating, 'r') as json_file:\n    result = [dict(json.loads(jline)) for jline in json_file.read().splitlines()]\n\ndocument = {}\nkeys_w = ['docid', 'title', 'authors', 'abstract', 'summary']\nfor i in range(len(result)):\n  temp = {}\n  for key in dict(result[i]):\n    if key in keys_w:\n      temp[key] = result[i][key]\n  document[result[i][\"docid\"]] = temp\n  # document[result[i][\"docid\"]] = dict(result[i])\nprint(type(document[result[0][\"docid\"]]))\nprint(\"Number of documents for testing:\",len(document))\nprint(\"Keys in the document dictionary:\",document[result[0]['docid']].keys())\n\n\n'''def parse_general_references(general_references):\n    title = []\n    for dict in general_references:\n      title.append(dict['title'])\n    return title\nfor value in document.values():\n    document[value['docid']]['general_references'] = parse_general_references(value[\"general_references\"])\n    '''\n#print(header)\nprint(\"Starting to write to File\")\nwriteTocsv(keys_wgaf)\nfor value in document.values():\n    writeTocsv(value.values())\nprint(\"Completed!!\")\n\ndef printJson(data):\n    print(json.dumps(data,indent=2))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:21.342413Z","iopub.execute_input":"2022-05-04T21:02:21.342948Z","iopub.status.idle":"2022-05-04T21:02:21.365767Z","shell.execute_reply.started":"2022-05-04T21:02:21.342915Z","shell.execute_reply":"2022-05-04T21:02:21.364738Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Removing PICO tags\nimport pandas as pd\nimport csv\nimport re\n\ndf = pd.read_csv(\"training.csv\")\n\nabs = df['abstract'].values\nprint('abstract', abs)\n\n\nstart = ['<pop>', '<int>', '<out>']\nend = ['</pop>', '</int>', '</out>']\nkeys = ['pop', 'int', 'out']\n\n\ndef writeTocsv(file, data):\n    with open(file, 'a+', encoding='UTF8', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(data)\n\n\n# The following function removes all PICO(HTML) tags from the data and keeps only the required string\ndef remove_PICO(abs_str):\n    for k in range(len(keys)):\n        summary[keys[k]] = re.findall(start[k]+\"(.*?)\"+end[k], abs_str, re.DOTALL)\n\n    \n    n = len(abs_str)\n    sent = abs_str\n    \n    for i in start:\n        if i in abs_str:\n            sent = sent.replace(i, '')\n\n    for i in end:\n        if abs_str:\n            sent = sent.replace(i, '')\n        \n    return sent","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:21.367162Z","iopub.status.idle":"2022-05-04T21:02:21.367808Z","shell.execute_reply.started":"2022-05-04T21:02:21.367598Z","shell.execute_reply":"2022-05-04T21:02:21.367620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read trainning data\ntrain_summary = pd.read_csv('../input/meddoc/training_summary.csv', encoding='iso-8859-1')\ntrain_text = pd.read_csv('../input/meddoc/training.csv', encoding='iso-8859-1')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:21.369314Z","iopub.status.idle":"2022-05-04T21:02:21.370017Z","shell.execute_reply.started":"2022-05-04T21:02:21.369835Z","shell.execute_reply":"2022-05-04T21:02:21.369855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read validation data\nval_summary = pd.read_csv('../input/meddoc/val_summary.csv', encoding='iso-8859-1')\nval_text = pd.read_csv('../input/meddoc/val.csv', encoding='iso-8859-1')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:21.390021Z","iopub.execute_input":"2022-05-04T21:02:21.390480Z","iopub.status.idle":"2022-05-04T21:02:21.425416Z","shell.execute_reply.started":"2022-05-04T21:02:21.390433Z","shell.execute_reply":"2022-05-04T21:02:21.424100Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Read test data\ntest_summary = pd.read_csv('../input/meddoc/test_summary.csv', encoding='iso-8859-1')\ntest_text = pd.read_csv('../input/meddoc/test.csv', encoding='iso-8859-1')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:21.426819Z","iopub.status.idle":"2022-05-04T21:02:21.427333Z","shell.execute_reply.started":"2022-05-04T21:02:21.427056Z","shell.execute_reply":"2022-05-04T21:02:21.427083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Concating text and summary for train,test and validation\ntrain_data=pd.concat([train_text,train_summary],ignore_index=True,axis=1)\nval_data=pd.concat([val_text,val_summary],ignore_index=True,axis=1)\ntest_data=pd.concat([test_text,test_summary],ignore_index=True,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:21.429493Z","iopub.status.idle":"2022-05-04T21:02:21.429990Z","shell.execute_reply.started":"2022-05-04T21:02:21.429735Z","shell.execute_reply":"2022-05-04T21:02:21.429761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:21.464757Z","iopub.execute_input":"2022-05-04T21:02:21.465046Z","iopub.status.idle":"2022-05-04T21:02:21.480586Z","shell.execute_reply.started":"2022-05-04T21:02:21.465015Z","shell.execute_reply":"2022-05-04T21:02:21.479326Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Dropping empty columns in dataset\ntrain_data=train_data.drop(columns=[2,4])\nval_data=val_data.drop(columns=[2,4])\ntest_data=test_data.drop(columns=[2,4])","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:21.499752Z","iopub.execute_input":"2022-05-04T21:02:21.500290Z","iopub.status.idle":"2022-05-04T21:02:21.521711Z","shell.execute_reply.started":"2022-05-04T21:02:21.500253Z","shell.execute_reply":"2022-05-04T21:02:21.520636Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Renaming the columns in dataset\ntrain_data=train_data.rename(columns={0: \"doc_id\", 1: \"title\",3:\"text\",5:\"summary\"})\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:21.525439Z","iopub.execute_input":"2022-05-04T21:02:21.526888Z","iopub.status.idle":"2022-05-04T21:02:21.542979Z","shell.execute_reply.started":"2022-05-04T21:02:21.526842Z","shell.execute_reply":"2022-05-04T21:02:21.542015Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"val_data=val_data.rename(columns={0: \"doc_id\", 1: \"title\",3:\"text\",5:\"summary\"})\ntest_data=test_data.rename(columns={0: \"doc_id\", 1: \"title\",3:\"text\",5:\"summary\"})","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:21.547606Z","iopub.execute_input":"2022-05-04T21:02:21.548269Z","iopub.status.idle":"2022-05-04T21:02:21.570953Z","shell.execute_reply.started":"2022-05-04T21:02:21.548227Z","shell.execute_reply":"2022-05-04T21:02:21.569434Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#Cleaning text for tags, and puntuation and quotiatons\nimport re           \nfrom bs4 import BeautifulSoup \nfrom nltk.corpus import stopwords   \nstop_words = set(stopwords.words('english')) \ndef text_cleaner(text):\n    newString = text.lower()\n    newString = BeautifulSoup(newString, \"lxml\").text\n    newString = re.sub(r'\\([^)]*\\)', '', newString)\n    newString = re.sub('\"','', newString) \n    newString = re.sub(r\"'s\\b\",\"\",newString)\n    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n    tokens = [w for w in newString.split() if not w in stop_words]\n    long_words=[]\n    for i in tokens:\n        if len(i)>=3:                  #removing short word\n            long_words.append(i)   \n    return (\" \".join(long_words)).strip()\n\n#cleaning texts of train, test and  validation calling the above function\ncleaned_train_text = []\ncleaned_val_data=[]\ncleaned_test_data=[]\nfor t in train_data['text']:\n    cleaned_train_text.append(text_cleaner(t))\nfor v in val_data['text']:\n    cleaned_val_text.append(text_cleaner(t))\nfor t in test_data['text']:\n    cleaned_test_text.append(text_cleaner(t))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:21.594487Z","iopub.execute_input":"2022-05-04T21:02:21.595460Z","iopub.status.idle":"2022-05-04T21:02:23.663265Z","shell.execute_reply.started":"2022-05-04T21:02:21.595371Z","shell.execute_reply":"2022-05-04T21:02:23.660634Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Cleaning summary for tags, and puntuation and quotiatons\ndef summary_cleaner(text):\n    newString = re.sub('\"','', text)    \n    newString = re.sub(r\"'s\\b\",\"\",newString)\n    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n    newString = newString.lower()\n    tokens=newString.split()\n    newString=''\n    for i in tokens:\n        if len(i)>1:                                 \n            newString=newString+i+' '  \n    return newString\n\n#cleaning summary of train, test and  validation calling the above function\ncleaned_train_summary = []\ncleaned_val_summary=[]\ncleaned_test_summary=[]\nfor t in train_data['summary']:\n    cleaned_train_summary.append(summary_cleaner(t))\nfor v in val_data['summary']:\n    cleaned_val_summary.append(summary_cleaner(t))\nfor t in test_data['summary']:\n    cleaned_test_summary.append(summary_cleaner(t))\n\n\ntrain_data['cleaned_train_text']=cleaned_train_text\ntrain_data['cleaned_train_summary']=cleaned_train_summary\nval_data['cleaned_val_text']=cleaned_val_text\nval_data['cleaned_val_summary']=cleaned_val_summary\ntest_data['cleaned_test_text']=cleaned_test_text\ntest_data['cleaned_test_summary']=cleaned_test_summary\ntrain_data['cleaned_train_summary'].replace('', np.nan, inplace=True)\nval_data['cleaned_val_summary'].replace('', np.nan, inplace=True)\ntest_data['cleaned_test_summary'].replace('', np.nan, inplace=True)\ntrain_data.dropna(axis=0,inplace=True)\nval_data.dropna(axis=0,inplace=True)\ntest_data.dropna(axis=0,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.664185Z","iopub.status.idle":"2022-05-04T21:02:23.665083Z","shell.execute_reply.started":"2022-05-04T21:02:23.664881Z","shell.execute_reply":"2022-05-04T21:02:23.664904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting wordcloud \ndef wc_blue_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n    return \"hsl(214, 67%%, %d%%)\" % randint(60, 100)\n\ndef plot_wordcloud(text, color_func):\n    wc_stopwords = get_wc_stopwords()\n    wc = WordCloud(stopwords=wc_stopwords, width=1200, height=600, random_state=0).generate(text)\n\n    f, axs = plt.subplots(figsize=(20, 10))\n    with sns.axes_style(\"ticks\"):\n        sns.despine(offset=10, trim=True)\n        plt.imshow(wc.recolor(color_func=color_func, random_state=0), interpolation=\"bilinear\")\n        plt.xlabel('WordCloud')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.665890Z","iopub.status.idle":"2022-05-04T21:02:23.666465Z","shell.execute_reply.started":"2022-05-04T21:02:23.666272Z","shell.execute_reply":"2022-05-04T21:02:23.666293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_wordcloud(' '.join(train_data['cleaned_summary_data'].values.tolist()), wc_blue_color_func)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.667449Z","iopub.status.idle":"2022-05-04T21:02:23.668094Z","shell.execute_reply.started":"2022-05-04T21:02:23.667898Z","shell.execute_reply":"2022-05-04T21:02:23.667920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adding start and end for each summary in train, test and validation to identify start and end of the texts\ntrain_data['cleaned_train_summary'] = train_data['cleaned_train_summary'].apply(lambda x : '_start_ '+ x + ' _end_')\nval_data['cleaned_val_summary'] = train_data['cleaned_val_summary'].apply(lambda x : '_start_ '+ x + ' _end_')\ntest_data['cleaned_test_summary'] = train_data['cleaned_test_summary'].apply(lambda x : '_start_ '+ x + ' _end_')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.669077Z","iopub.status.idle":"2022-05-04T21:02:23.669774Z","shell.execute_reply.started":"2022-05-04T21:02:23.669579Z","shell.execute_reply":"2022-05-04T21:02:23.669598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(5):\n    print(\"Review:\",train_data['cleaned_train_text'][i])\n    print(\"Summary:\",train_data['cleaned_train_summary'][i])\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.670913Z","iopub.status.idle":"2022-05-04T21:02:23.671214Z","shell.execute_reply.started":"2022-05-04T21:02:23.671058Z","shell.execute_reply":"2022-05-04T21:02:23.671075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ttext_count = []\ntsummary_count = []","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.672087Z","iopub.status.idle":"2022-05-04T21:02:23.672713Z","shell.execute_reply.started":"2022-05-04T21:02:23.672495Z","shell.execute_reply":"2022-05-04T21:02:23.672516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sent in train_data['cleaned_train_text']:\n    ttext_count.append(len(sent.split()))\nfor sent in train_data['cleaned_train_summary']:\n    tsummary_count.append(len(sent.split()))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.673538Z","iopub.status.idle":"2022-05-04T21:02:23.673883Z","shell.execute_reply.started":"2022-05-04T21:02:23.673700Z","shell.execute_reply":"2022-05-04T21:02:23.673730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_wordcount_df= pd.DataFrame()\nt_wordcount_df['train_text']=ttext_count\nt_wordcount_df['train_summary']=tsummary_count","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.674733Z","iopub.status.idle":"2022-05-04T21:02:23.675028Z","shell.execute_reply.started":"2022-05-04T21:02:23.674874Z","shell.execute_reply":"2022-05-04T21:02:23.674889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot histogram to check range of words in text and summary in train data\nimport matplotlib.pyplot as plt\n\ntwordcount_df.hist(bins = 5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.676014Z","iopub.status.idle":"2022-05-04T21:02:23.676323Z","shell.execute_reply.started":"2022-05-04T21:02:23.676167Z","shell.execute_reply":"2022-05-04T21:02:23.676184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To count number of words in train summary\ncnt=0\nfor i in train_data['cleaned_train_summary']:\n    if(len(i.split())<=300):\n        cnt=cnt+1\nprint(cnt/len(train_data['cleaned_train_summary']))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.677854Z","iopub.status.idle":"2022-05-04T21:02:23.678345Z","shell.execute_reply.started":"2022-05-04T21:02:23.678161Z","shell.execute_reply":"2022-05-04T21:02:23.678181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To count number words in train text\ncnt=0\nfor i in train_data['cleaned_train_text']:\n    if(len(i.split())<=2000):\n        cnt=cnt+1\nprint(cnt/len(train_data['cleaned_train_text']))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.679318Z","iopub.status.idle":"2022-05-04T21:02:23.679880Z","shell.execute_reply.started":"2022-05-04T21:02:23.679676Z","shell.execute_reply":"2022-05-04T21:02:23.679723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Setting text and summary words limit\nmax_text_len=6000\nmax_summary_len=60","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.680728Z","iopub.status.idle":"2022-05-04T21:02:23.681391Z","shell.execute_reply.started":"2022-05-04T21:02:23.681206Z","shell.execute_reply":"2022-05-04T21:02:23.681226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Select the Summaries and Text between max len defined above\n\ncleaned_train_text =np.array(train_data['cleaned_train_text'])\ncleaned_train_summary=np.array(train_data['cleaned_train_summary'])\ncleaned_val_text =np.array(train_data['cleaned_val_text'])\ncleaned_val_summary=np.array(train_data['cleaned_val_summary'])\ncleaned_test_text =np.array(train_data['cleaned_test_text'])\ncleaned_test_summary=np.array(train_data['cleaned_test_summary'])\n\ndef shorten(cleaned_data,max_text_len,max_summary_len):\ntrain_text_small=[]\ntrain_summary_small=[]\n\nfor i in range(len(cleaned_data)):\n    if(len(cleaned_train_summary[i].split())<=max_summary_len and len(cleaned_train_text[i].split())<=max_text_len):\n        train_text_small.append(cleaned_train_text[i])\n        train_summary_small.append(cleaned_train_summary[i])\nreturn train_text_small,train_summary_small\n               \ntrain_text_small,train_summary_small=shorten(cleaned_train_text)        \nst_data=pd.DataFrame({'text':train_text_small,'summary':train_summary_small})\nval_text_small,val_summary_small=shorten(cleaned_val_text)        \nsv_data=pd.DataFrame({'text':val_text_small,'summary':val_summary_small})\ntest_text_small,test_summary_small=shorten(cleaned_test_text)        \nstt_data=pd.DataFrame({'text':test_text_small,'summary':test_summary_small})","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.682411Z","iopub.status.idle":"2022-05-04T21:02:23.682981Z","shell.execute_reply.started":"2022-05-04T21:02:23.682811Z","shell.execute_reply":"2022-05-04T21:02:23.682830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adding sostok and eostok at the end of summary\nst_data['summary'] = st_data['summary'].apply(lambda x : 'sostok '+ x + ' eostok')\nsv_data['summary'] = sv_data['summary'].apply(lambda x : 'sostok '+ x + ' eostok')\nstt_data['summary'] = stt_data['summary'].apply(lambda x : 'sostok '+ x + ' eostok')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.683961Z","iopub.status.idle":"2022-05-04T21:02:23.684487Z","shell.execute_reply.started":"2022-05-04T21:02:23.684304Z","shell.execute_reply":"2022-05-04T21:02:23.684324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train=st_data['text']\ny_train=st_data['summary']\nx_val=sv_data['text']\ny_val=sv_data['summary']\nx_test=stt_data['text']\ny_test=stt_data['summary']","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.685789Z","iopub.status.idle":"2022-05-04T21:02:23.686412Z","shell.execute_reply.started":"2022-05-04T21:02:23.686127Z","shell.execute_reply":"2022-05-04T21:02:23.686158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tokenizing to get vocabulary\n\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\n\nx_tokenizer = Tokenizer() \nx_tokenizer.fit_on_texts(list(x_train))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.687673Z","iopub.status.idle":"2022-05-04T21:02:23.688153Z","shell.execute_reply.started":"2022-05-04T21:02:23.687895Z","shell.execute_reply":"2022-05-04T21:02:23.687921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold=4\ncount=0\ntotal=0\nfrequent=0\ntotal_f=0\n\nfor key,value in x_tokenizer.word_counts.items():\n    total_f=total_f+value\n    total=total+1\n    if(threshold>value):\n        count=count+1\n        frequent=frequent+value\n    \nprint(\"% of rare words in vocabulary:\",(count/total)*100)\nprint(\"Total Coverage of rare words:\",(frequent/total_f)*100)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.689516Z","iopub.status.idle":"2022-05-04T21:02:23.689998Z","shell.execute_reply.started":"2022-05-04T21:02:23.689737Z","shell.execute_reply":"2022-05-04T21:02:23.689764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tokening training data text\nx_tokenizer = Tokenizer(num_words=total-count) \nx_tokenizer.fit_on_texts(list(x_train))\n\n\n#one-hot encodeing text to word\nx_tr_seq    =   x_tokenizer.texts_to_sequences(x_train) \nx_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n#padding zero \nx_train    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\nx_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n\n\n\n#Vocabulary+1 for padding\nx_vocab   =  x_tokenizer.num_words + 1\n\nprint(\"Size of vocabulary in X = {}\".format(x_vocab))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.691077Z","iopub.status.idle":"2022-05-04T21:02:23.691540Z","shell.execute_reply.started":"2022-05-04T21:02:23.691283Z","shell.execute_reply":"2022-05-04T21:02:23.691309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_tokenizer = Tokenizer()   \ny_tokenizer.fit_on_texts(list(y_train))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.694548Z","iopub.status.idle":"2022-05-04T21:02:23.695066Z","shell.execute_reply.started":"2022-05-04T21:02:23.694788Z","shell.execute_reply":"2022-05-04T21:02:23.694816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold=6\ncount=0\ntotal=0\nfrequent=0\ntotal_f=0\n\nfor key,value in y_tokenizer.word_counts.items():\n    total_f=total_f+value\n    total=total+1\n    if(threshold>value):\n        count=count+1\n        frequent=frequent+value\n    \nprint(\"% of rare words in vocabulary:\",(count/total)*100)\nprint(\"Total Coverage of rare words:\",(frequent/total_f)*100)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.696574Z","iopub.status.idle":"2022-05-04T21:02:23.697056Z","shell.execute_reply.started":"2022-05-04T21:02:23.696807Z","shell.execute_reply":"2022-05-04T21:02:23.696834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tokening training data summary\ny_tokenizer = Tokenizer(num_words=total-count) \ny_tokenizer.fit_on_texts(list(y_train))\n\n#one-hot encodeing text to word\ny_tr_seq    =   y_tokenizer.texts_to_sequences(y_train) \ny_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n\n#padding zero \ny_train    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\ny_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n\n#Vocabulary+1 for padding\ny_vocab  =   y_tokenizer.num_words +1\nprint(\"Size of vocabulary in Y = {}\".format(y_vocab))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.698267Z","iopub.status.idle":"2022-05-04T21:02:23.698743Z","shell.execute_reply.started":"2022-05-04T21:02:23.698475Z","shell.execute_reply":"2022-05-04T21:02:23.698501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K \nimport gensim\nfrom numpy import *\nimport numpy as np\nimport pandas as pd \nimport re\nfrom bs4 import BeautifulSoup\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport warnings\npd.set_option(\"display.max_colwidth\", 200)\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Size of vocabulary from the w2v model = {}\".format(x_voc))\n\nK.clear_session()\n\nlatent_dim = 300\nembedding_dim=200\n\n# Encoder\nencoder_inputs = Input(shape=(max_text_len,))\n\n#embedding layer\nenc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n\n#encoder lstm 1\nencoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n\n#encoder lstm 2\nencoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n\n#encoder lstm 3\nencoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\n\n\n#embedding layer\ndec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\ndec_emb = dec_emb_layer(decoder_inputs)\n\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\ndecoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n\n#dense layer\ndecoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Model definition\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.700552Z","iopub.status.idle":"2022-05-04T21:02:23.701063Z","shell.execute_reply.started":"2022-05-04T21:02:23.700801Z","shell.execute_reply":"2022-05-04T21:02:23.700829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Compiling the model\nmodel.compile(optimizer='sgd', loss='sparse_categorical_crossentropy')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.702732Z","iopub.status.idle":"2022-05-04T21:02:23.703190Z","shell.execute_reply.started":"2022-05-04T21:02:23.702944Z","shell.execute_reply":"2022-05-04T21:02:23.702969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fitting the model\nhistory=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=20,batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))\nmodel.save('model_1.h5', save_format = 'h5')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.704472Z","iopub.status.idle":"2022-05-04T21:02:23.704955Z","shell.execute_reply.started":"2022-05-04T21:02:23.704705Z","shell.execute_reply":"2022-05-04T21:02:23.704731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Plotting the graph for train and validation loss\nfrom matplotlib import pyplot\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.705983Z","iopub.status.idle":"2022-05-04T21:02:23.706450Z","shell.execute_reply.started":"2022-05-04T21:02:23.706192Z","shell.execute_reply":"2022-05-04T21:02:23.706217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Rexersing and getting back the string form of the summary\nreverse_target_word_index=y_tokenizer.index_word \nreverse_source_word_index=y_tokenizer.index_word \ntarget_word_index=y_tokenizer.word_index","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.708360Z","iopub.status.idle":"2022-05-04T21:02:23.708848Z","shell.execute_reply.started":"2022-05-04T21:02:23.708568Z","shell.execute_reply":"2022-05-04T21:02:23.708594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoder Inference\nencoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n\n# Decoder Inference\n\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_hidden_state_input = Input(shape=(max_len_body,latent_dim))\n\n# Getting decoder sequence embeddings\ndec_emb2= dec_emb_layer(decoder_inputs)\n\n# Predicting the next word in the sequence\n\n\n\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n\n# Dense softmax layer to calculate probability distribution over target vocab\ndecoder_outputs2 = decoder_dense(decoder_outputs2)\n\n# Final model\ndecoder_model = Model(\n[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n[decoder_outputs2] + [state_h2, state_c2])","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.710642Z","iopub.status.idle":"2022-05-04T21:02:23.711129Z","shell.execute_reply.started":"2022-05-04T21:02:23.710879Z","shell.execute_reply":"2022-05-04T21:02:23.710906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_sequence(input_seq):\n    # Encoding input as state vectors\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n\n    # Initializing target to 1\n    target_seq = np.zeros((1,1))\n\n    # Taking the 'start' word as the first word of the target sequence\n    target_seq[0, 0] = target_word_index['start']\n\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        try:\n            sampled_token = reverse_target_word_index[sampled_token_index]\n        except:\n            sampled_token = reverse_target_word_index[np.random.randint(1, len(reverse_target_word_index))]\n        if(sampled_token!='end'):\n            decoded_sentence += ' '+sampled_token\n\n            # Stop when max length is reached or find end word.\n            if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_len_highlight-1)):\n                stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        e_h, e_c = h, c\n\n    return decoded_sentence","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.712530Z","iopub.status.idle":"2022-05-04T21:02:23.713037Z","shell.execute_reply.started":"2022-05-04T21:02:23.712783Z","shell.execute_reply":"2022-05-04T21:02:23.712809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convet texts back to strings(decode)\ndef seq2text(input_sequence):\n    new_string = ''\n    for i in input_sequence:\n        if i != 0:\n            new_string = new_string + reverse_source_word_index[i] + ' '\n    return new_string","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.714724Z","iopub.status.idle":"2022-05-04T21:02:23.715058Z","shell.execute_reply.started":"2022-05-04T21:02:23.714898Z","shell.execute_reply":"2022-05-04T21:02:23.714916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert summary back to string(decode)\ndef seq2summary(input_sequence):\n    new_string = ''\n    for i in input_sequence:\n        if (\n            (i != 0 and i != target_word_index[start_token]) and\n            (i != target_word_index[end_token])\n        ):\n            new_string = new_string + reverse_target_word_index[i] + ' '\n    return new_string","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.716162Z","iopub.status.idle":"2022-05-04T21:02:23.716481Z","shell.execute_reply.started":"2022-05-04T21:02:23.716318Z","shell.execute_reply":"2022-05-04T21:02:23.716336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Storing original and predicted to lists\noriginal_summary=[]\npredicted_summary=[]\nfor i in range(len(x_test)):\n  original_summary.append(seq2text(x_test[i]))\n  predicted summary.append(decode_sequence(x_test[i].reshape(1,max_summary_len)))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.717349Z","iopub.status.idle":"2022-05-04T21:02:23.717637Z","shell.execute_reply.started":"2022-05-04T21:02:23.717486Z","shell.execute_reply":"2022-05-04T21:02:23.717502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Writing original a predicted summary to a csv \nimport csv\n\nwith open('./predict.csv', 'w') as f:\n    writer = csv.writer(f)\n    writer.writerows(zip(original_summary,predicted_summary))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.718496Z","iopub.status.idle":"2022-05-04T21:02:23.718863Z","shell.execute_reply.started":"2022-05-04T21:02:23.718633Z","shell.execute_reply":"2022-05-04T21:02:23.718649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge\nimport json\nfrom rouge import Rouge\nimport pandas as pd\nimport numpy as np\n\n#Reading the csv and writing original and predicted to lists\ndata=pd.read_csv('../input/sgd-evaluation/some1 (4).csv')\noriginal=data['original_summary'].tolist()\npredicted=data['predicted_summary'].tolist()\n\n#Calculating the Rouge scores\nrouge = Rouge()\nscores = rouge.get_scores(original,predicted, avg=True)\n\nprint(scores)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.719827Z","iopub.status.idle":"2022-05-04T21:02:23.720122Z","shell.execute_reply.started":"2022-05-04T21:02:23.719967Z","shell.execute_reply":"2022-05-04T21:02:23.719983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\nactual=[]\npredicted=[]\n#Spliting the words in each summary to perform bleu evaluataion\nfor i in range(len(original)):\n    actual.append([original[i].split()])\n    predicted.append(predicted[i].split())\n\n#Finding bleu for 1,2,3 and 4 shingles\nbleu = {}\nbleu['1-grams'] = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\nbleu['2-grams'] = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\nbleu['3-grams'] = corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))\nbleu['4-grams'] = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n\nprint(bleu)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:02:23.721324Z","iopub.status.idle":"2022-05-04T21:02:23.721614Z","shell.execute_reply.started":"2022-05-04T21:02:23.721462Z","shell.execute_reply":"2022-05-04T21:02:23.721477Z"},"trusted":true},"execution_count":null,"outputs":[]}]}